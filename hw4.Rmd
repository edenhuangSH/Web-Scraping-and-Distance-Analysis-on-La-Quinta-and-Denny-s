---
title: "Homework 4"
author: Faustine, Eden, Shaoji, Yumemichi
output: html_document
---
<br>

> "La Quinta is Spanish for next to Dennyâ€™s" - Mitch Hedberg

Our group scraped location data from La Quinta and Denny's websites to see if this joke has a kernal of truth in it. 

### Task 1 - Scraping La Quinta
write up:

1. Write up for get_lq.R:

For doing this task, set the site as the mirror La Quinta website and url for the web page that shows all the web link for individual hotel pages. 
Then we use R package, `rvest`, and use `html_read()` function to read the url address of the mirror La Quinta website that shows the indicidual hotel pages, and use both pipe and `html_nodes()` function to extract individual hotel page urls,  get all of the hotels regarding the location of them. Then download all of the individual websites, and stripping out the NA(missing value) using na.omit. After this, create a directory using `dir_create`, and add "showWarnings = False "so that we don't need to show the warnings again and again. Finally, use a for loop to download the hotel page for each lq location, and use filepath to put the destfile in the directory.

2. Write up for parse_lq.R:

In this R file, we use `rvest`, `stringr`, `tibble`, `dplyr` and `method` packages. 
We create a file and, look at the directory we created in get_lq.R, and find all the html file there by using `dir()`, saving the result as a list.
Then, parse the lq html files and extract information using a for loop. Inside the for loop, we read the every file in the files, using `html_nodes()` to extract address, Phone and Fax information. For getting the tag for `html_nodes()`, we use SelectorGadget, and then by piping and using `html_text()` we can get teh text of teh information, and then we use `str_split()` for this vector, since we want to split the information to Address, Phone number and Fax number, and inside the vector we notice that "\n" is the thing we want to split, so we put this in the `str_split()` function. Then, after we split the information into different lines, we get rid of the list and use `str_trim()` to get rid of the extra space. For getting information of location name, room numbers and floor numbers, we use basically teh same way as getting information for hotel information. For latitude and longitude, we find that the Google link of the small map in the website includes latitude first then longitude, so we use `str_match()` to get the number we want. For the availabilities of Internet acees and swimming pool, we first get the whole information of all amenities and sevices and then we use `str_detect() ` to find if there is swimming pool and internet acces or not, and if there is, the functon will return TURE, if there is not, teh function wil return FALSE. Finally, we put all the information(location name, address, phone, fax, number of rooms, latitude, longtitude, number of floors, swimming pool and internet access) in a data frame, store in the result list.
 
We also create a Mkefile to link the R script together.

### Task 2 - Scraping Denny's

The location of each Denny's was obtained by interacting with a third-party location service - this is the service that powers the location map and search feature on the Denny's website. In this case we queried the location provider "Where 2 Get It".

The `get_dennys` script takes the form of an xml request to the where2getit domain. In that request, we first translated the url to the form that was more interpreble. We found three important xml calls: `zipcode`, `radius`, and `limit`. 

The `zipcode` value is a US zipcode that gives us a center point. The `radius` is the maximum distance in miles that the locator looks out from the zipcode. Limit gives the maxiumum number of resturants to return. 

We set the `zipcodes` strategically to cover the entire US. We picked locations in the East and West coast to cover the continetal US. We also picked locations in Alaska and Hawaii to cover all 50 states. The four cities we choose were Washington DC, Los Angeles California, Honolulu HI, and Fairbanks AK. Zipcodes for these cities were found on Wikipedia. 

We wanted the `radius` value to be the smallest values that cover all the locations. We increased the radius until the number of locations stopped increasing. For the four cities we choose, that number happened to be `radius = 5000`. Finally, we set the limit to be `10000`, which is much higher than the number of Denny's locations so that we could be sure we aren't missing some values. 

Finally, we downloaded the resulting xml files with the infomation of Denny's locations in the `data/dennys` folder.

We parsed the Denny's xml files similarly to the La Quinta data. We read the xml file using `rvest` and used the `xml_text()` to extract the text. The location information is in the `<latitude>` and `longitude` node. We also parsed the state data. By looking at the state data we see that we got all the Denny's from 51 states (50 states plus Washington DC).


### Task 3 - Distance Analysis

```{r}
load('data/lq.Rdata')
load('data/dennys.Rdata')
library('maps')
library('datasets')
library('ggplot2')
library('dplyr')


# Build data frames for La Quinta and Denny's that include state, lattitude and longitude
lqLoc = data.frame(State = hotels$state, 
                   Lattitude = as.numeric(hotels$lat), 
                   Longitude = as.numeric(hotels$long))
dennysLoc = data.frame(State = dennys$state, 
                   Lattitude = as.numeric(dennys$lat), 
                   Longitude = as.numeric(dennys$long))

#  Remove the denny's and La Quinta with no State, which means they are outside of US
lqLoc = lqLoc[!is.na(lqLoc$State),]
dennysLoc = dennysLoc[dennysLoc$State != "",]

# Re-order the data frame by state
lqLoc = lqLoc[order(lqLoc$State),]
dennysLoc = dennysLoc[order(dennysLoc$State),]

# Find the states where both Denny's and La Quinta exists
lq_states = unique(lqLoc$State)
dennys_states = unique(dennysLoc$State)
common_states = intersect(lq_states, dennys_states)

# Selecting Denny's and La Quita that exist in states having both of them
dennys_commonStates = dennysLoc[dennysLoc$State %in% common_states,]
lq_commonStates = lqLoc[lqLoc$State %in% common_states,]

# Plot Denny's and La Quinta on the map
# Make a new data frame that stores all La Quinta and Denny's information for ggplot
lq_melt = data.frame(Type = rep("La Quinta", nrow(lq_commonStates))) %>%
  cbind(lq_commonStates)
dennys_melt = data.frame(Type = rep("Denny's", nrow(dennys_commonStates))) %>%
  cbind(dennys_commonStates)
lq_dennys = rbind(lq_melt,dennys_melt)
lq_dennys$Type = as.factor(lq_dennys$Type)

# Create a data frame that stores information of all US states
world = map_data("world")
Alaska = world[world$subregion == "Alaska",] %>%
  .[!is.na(.$subregion),]
Alaska$region = Alaska$subregion
Alaska$subregion = rep(NA, nrow(Alaska))
all_states = rbind(map_data("state"), Alaska)

# Do the plot
ggplot() + 
  geom_polygon(data = all_states, aes(x=long, y=lat, group = group), colour="white", fill="blue") +
  xlim(-180, -60) + 
  geom_point(data = lq_dennys, aes(x=Longitude, y=Lattitude, group=Type, color=Type), cex = 0.6)


# Calculate the distance between each of the Denny's and La Quinta in the common_states
# formula: spherical law of cosines
# reference: http://www.movable-type.co.uk/scripts/latlong.html
res = list()
for(i in 1:nrow(hotels)) {
  lat1 = rep(lq_commonStates$Lattitude[i], nrow(dennys_commonStates))
  lon1 = rep(lq_commonStates$Longitude[i], nrow(dennys_commonStates))
  lat2 = dennys_commonStates$Lattitude
  lon2 = dennys_commonStates$Longitude
  res[[i]] = acos(sin(lat1*pi/180)*sin(lat2*pi/180)+
           cos(lat1*pi/180)*cos(lat2*pi/180)*cos(lon2*pi/180-lon1*pi/180)) * 6371000
}

counts = c()
for(i in 1:nrow(hotels)) {
  counts = c(counts, sum(res[[i]] < 150))
}
counts
```

### Discussion 


